# Tema 4. Procesamiento de lenguaje natural y LLM (Large Language Models)

## Limpieza y preprocesamiento de textos

1. Introducci√≥n
1. Limpieza de textos
1. Preprocesamiento de textos

---

### Introducci√≥n

En cualquier proyecto de procesamiento del lenguaje natural (PLN), antes de llegar a la codificaci√≥n del texto, es fundamental pasar por una serie de fases de **limpieza** y **preprocesamiento**. Estas etapas aseguran que los datos est√©n en un formato adecuado para el an√°lisis y que se reduzca el ruido, maximizando la utilidad del texto para los modelos de aprendizaje autom√°tico. El resto de este cap√≠tulo se dedicar√° a explicar cu√°les son cada una de las mencionadas fases y de qu√© manera podemos implementarlas desde un punto de vista pr√°ctico. Pero primeramente enumeremos las fases en las que podemos dividir un proceso de limpieza y preprocesamiento de textos:

En lo que respecta a la **limpieza del texto** podemos distinguir entre:

1. **Eliminaci√≥n de caracteres no deseados**: Eliminaci√≥n de signos de puntuaci√≥n, caracteres especiales, n√∫meros u otros elementos irrelevantes para la tarea.
2. **Conversi√≥n a min√∫sculas**: Normalizaci√≥n del texto para evitar que palabras en distinto caso sean tratadas como diferentes.
3. **Eliminaci√≥n de espacios innecesarios**: Eliminaci√≥n de espacios adicionales que puedan generar inconsistencias en los datos.
4. **Correcci√≥n ortogr√°fica (opcional)**: En tareas espec√≠ficas, se pueden corregir errores ortogr√°ficos para mejorar la calidad del texto.

En lo que respecta al **preprocesamiento del texto** podemos distinguir entre:

1. **Eliminaci√≥n de  stop-words**: eliminaci√≥n de palabras muy frecuentes y con poca carga sem√°ntica.
2. **Tokenizaci√≥n**: Divisi√≥n del texto en unidades b√°sicas, ya sean palabras, frases o caracteres. Ello depender√° de la tarea concreta a abordar.
3. **Lematizaci√≥n o stemming**: Reducci√≥n de las palabras a su forma base o ra√≠z para normalizar el vocabulario.
4. **Normalizaci√≥n adicional (opcional)**: Sustituci√≥n de palabras sin√≥nimas o aplicaci√≥n de t√©cnicas espec√≠ficas que reduzcan la complejidad del texto.
5. **Filtrado de t√©rminos irrelevantes**: En algunos casos, es √∫til eliminar palabras espec√≠ficas que no sean √∫tiles para la tarea en cuesti√≥n (como nombres propios o t√©rminos t√©cnicos irrelevantes).

Todas las fases enumeradas anteriormente deben ser realizadas en el orden adecuado y adaptadas a los requisitos espec√≠ficos del problema que se est√° resolviendo. Una vez completadas, el texto estar√° listo para ser codificado en un formato num√©rico que los modelos puedan interpretar.

### Eliminaci√≥n de caracteres no deseados

La **eliminaci√≥n de caracteres no deseados** es una de las primeras fases en el proceso de limpieza de texto en proyectos de procesamiento del lenguaje natural (PLN). Este paso tiene como objetivo eliminar caracteres y elementos concretos que no aportan informaci√≥n relevante para la tarea espec√≠fica, reduciendo el ruido y asegurando que el texto est√© en un formato adecuado para su posterior procesamiento. Estos caracteres incluyen, entre otros, signos de puntuaci√≥n, caracteres especiales, n√∫meros (en algunos casos), y secuencias de espacios en blanco. De todos modos, es la naturaleza de la tarea la que marcar√° las necesidades en cuanto a la eliminaci√≥n o no de caracteres concretos y s√≠mbolos.

En efecto, aunque a primera vista puede parecer un paso sencillo, la eliminaci√≥n de caracteres no deseados debe realizarse con cuidado, ya que puede haber informaci√≥n valiosa escondida en algunos de ellos seg√∫n el contexto y la tarea. Por ejemplo, mientras los signos de puntuaci√≥n pueden ser irrelevantes en un an√°lisis de sentimientos, podr√≠an ser importantes en la identificaci√≥n de entidades nombradas o en la generaci√≥n de texto.

> **Ejemplo 1:** An√°lisis de sentimientos en redes sociales
>
> El objetivo en este caso es determinar el sentimiento expresado en publicaciones de redes sociales. Las publicaciones pueden incluir emojis, signos de puntuaci√≥n y hashtags, que en este contexto **aportan informaci√≥n valiosa sobre el sentimiento**. Supongamos el siguiente texto a procesar
>
> ```
> "¬°Me encanta este producto! üòä‚ù§Ô∏è #felicidad #compras"
> ```
>
> En este caso se podr√≠a proceder de la siguiente manera:
>
> 1. Conservar emojis y hashtags porque aportan informaci√≥n emocional.
> 2. Eliminar signos de puntuaci√≥n innecesarios para simplificar el texto.
>
> De este modo, tras la limpieza, el texto quedar√≠a como:
>
> ```
> "Me encanta este producto üòä‚ù§Ô∏è felicidad compras"
> ```
>
> Vemos como en este caso, los emojis (üòä, ‚ù§Ô∏è) y los hashtags (#felicidad) son importantes porque a√±aden se√±ales emocionales y tem√°ticas. Eliminarlos podr√≠a llevar a perder informaci√≥n relevante para el an√°lisis de sentimientos.

> **Ejemplo2:** Procesamiento de textos financieros
>
> En este caso, el objetivo es extraer informaci√≥n clave de documentos financieros, como contratos o informes. Aqu√≠, elementos como n√∫meros, s√≠mbolos de moneda ($, ‚Ç¨, ¬•) y porcentajes (%) **son esenciales para la interpretaci√≥n correcta** del texto.
>
> Supongamos el siguiente texto:
>
> ```
> "El valor total del contrato es de $1,200,000 con una tasa de inter√©s del 5% anual."
> ```
>
> Los criterios a seguir podr√≠an ser:
>
> 1. Conservar n√∫meros y s√≠mbolos de moneda porque son cr√≠ticos para la tarea.
> 2. Eliminar signos de puntuaci√≥n innecesarios, como comas separadoras, para estandarizar el formato num√©rico.
>
> De este modo el texto quedar√≠a finalmente:
>
> ```
> "El valor total del contrato es de 1200000 con una tasa de inter√©s del 5% anual"
> ```
>
> En este caso, eliminar n√∫meros o s√≠mbolos de moneda como "$" habr√≠a afectado la interpretaci√≥n financiera del texto. Por el contrario, quitar las comas de los n√∫meros ayuda a mantener consistencia en el formato.

#### T√©cnicas para eliminar caracteres no deseados

##### Uso de expresiones regulares

Las **expresiones regulares** son herramientas extremadamente poderosas para buscar y manipular patrones en el texto. Permiten identificar y eliminar caracteres espec√≠ficos o grupos de caracteres seg√∫n reglas definidas.

Por ejemplo:

- Para eliminar todos los signos de puntuaci√≥n: `r'[^\w\s]'` (este patr√≥n elimina todo excepto palabras y espacios).
- Para eliminar n√∫meros: `r'\d+'`.
- Para eliminar caracteres no alfab√©ticos: `r'[^a-zA-Z\s]'`.

El uso de expresiones regulares es altamente configurable, lo que las hace ideales para tareas avanzadas.

##### Sustituci√≥n por listas predefinidas

Otra t√©cnica consiste en definir expl√≠citamente los caracteres a eliminar o conservar. Esto puede ser √∫til si se tiene un control m√°s estricto sobre el tipo de texto procesado. Por ejemplo, en un corpus t√©cnico, se podr√≠a decidir conservar ciertos caracteres especiales como "#" o "$" porque tienen un significado relevante.

##### Uso de bibliotecas especializadas en python

Python ofrece herramientas poderosas para la limpieza de texto, como las expresiones regulares a trav√©s del m√≥dulo `re`, y bibliotecas espec√≠ficas como `nltk` , `spacy` o incluso `pandas`. 

La eliminaci√≥n de caracteres no deseados es un paso crucial en la limpieza de texto y cada una de estas bibliotecas ofrece funcionalidades espec√≠ficas que se pueden adaptar seg√∫n los requisitos de la tarea y el contexto del an√°lisis.

La biblioteca **re** de Python, basada en expresiones regulares, es una de las opciones m√°s flexibles y poderosas para identificar y eliminar patrones espec√≠ficos en un texto. Con re, se pueden eliminar signos de puntuaci√≥n, n√∫meros, espacios en blanco redundantes o incluso caracteres especiales en una sola l√≠nea de c√≥digo. Por ejemplo, `re.sub(r'[^\w\s]', '', texto)` elimina todos los caracteres que no sean palabras o espacios, dejando un texto limpio y estructurado.

Por otro lado, **nltk** ofrece herramientas para una limpieza m√°s sem√°ntica. Aunque su enfoque principal est√° en el preprocesamiento, como la tokenizaci√≥n y la eliminaci√≥n de  stop-words, tambi√©n puede usarse para filtrar texto mediante listas predefinidas de caracteres no deseados o  stop-words espec√≠ficas. Esto resulta √∫til cuando se necesita un control m√°s granular en textos en m√∫ltiples idiomas o dominios.

Finalmente, **spaCy** combina eficiencia y simplicidad al proporcionar m√©todos preintegrados para manipular texto. Por ejemplo, al procesar un documento con spaCy, se pueden eliminar caracteres no deseados aprovechando sus etiquetas morfol√≥gicas, como distinguir entre puntuaci√≥n, n√∫meros y palabras relevantes. Adem√°s, spaCy permite un an√°lisis m√°s profundo del texto, como conservar palabras clave o s√≠mbolos importantes seg√∫n su uso en el corpus.

En conjunto, estas bibliotecas ofrecen enfoques complementarios, adaptables a las necesidades de cualquier proyecto de PLN. Su correcta integraci√≥n garantiza una limpieza eficiente y personalizada del texto, sentando las bases para an√°lisis precisos y efectivos.

> [!note]
>
> La eliminaci√≥n de caracteres no deseados es un paso cr√≠tico en la limpieza de texto. Su correcta implementaci√≥n asegura que el texto est√© libre de ruido y listo para las etapas posteriores de preprocesamiento y an√°lisis. Dependiendo del contexto y la tarea, las t√©cnicas deben ajustarse cuidadosamente para no perder informaci√≥n relevante. Herramientas como expresiones regulares y bibliotecas de Python como `re`, `nltk` o `spacy` permiten realizar esta tarea de manera eficiente, incluso en corpus de gran tama√±o. Este paso, aunque inicial, sienta las bases para un procesamiento del lenguaje m√°s preciso y efectivo.

### Unificaci√≥n del caso (conversi√≥n a min√∫sculas o may√∫sculas)

La **conversi√≥n o unificaci√≥n del caso**, es decir, transformar todo el texto a min√∫sculas o may√∫sculas, es una de las operaciones m√°s b√°sicas pero esenciales en el proceso de limpieza de texto en proyectos de procesamiento del lenguaje natural (PLN). Este paso tiene como objetivo normalizar el texto, garantizando, por ejemplo, que palabras como "Casa" y "casa" no sean tratadas como diferentes por los algoritmos. Aunque las letras may√∫sculas pueden aportar significado en ciertos contextos, como nombres propios o siglas, para muchas tareas de PLN esta distinci√≥n no es relevante, y la conversi√≥n a min√∫sculas simplifica el an√°lisis y mejora la consistencia de los datos.

El valor de esta etapa depende de la tarea espec√≠fica. En proyectos como an√°lisis de sentimientos, clasificaci√≥n de texto o generaci√≥n de lenguaje, la conversi√≥n a min√∫sculas es casi siempre √∫til, ya que evita que el modelo tenga que aprender m√∫ltiples representaciones para palabras id√©nticas, como "Apple" y "apple". Sin embargo, en tareas como la extracci√≥n de entidades nombradas, puede ser preferible conservar las may√∫sculas, ya que aportan informaci√≥n sobre nombres propios, t√≠tulos o siglas.

> **Ejemplo 1**: An√°lisis de sentimientos en rese√±as de productos
>
> En un an√°lisis de sentimientos, el objetivo es determinar si el texto refleja una opini√≥n positiva, negativa o neutral. En este caso, las may√∫sculas no aportan informaci√≥n adicional sobre el sentimiento.
>
> Supongamos el siguiente texto
>
> ```
> "¬°Este Producto es Fant√°stico! Muy recomendable."
> ```
>
> Se podr√≠a convertir todo el texto a min√∫sculas para evitar redundancias en las representaciones de palabras. De este modo el texto final quedar√≠a como:
>
> ```
> "¬°este producto es fant√°stico! muy recomendable."
> ```

> **Ejemplo 2:** Extracci√≥n de entidades nombradas (NER)
>
> En tareas de extracci√≥n de entidades nombradas, como identificar nombres de personas, empresas o ubicaciones, conservar las may√∫sculas puede ser crucial, ya que estas suelen ser un indicador de nombres propios. Por ejemplo, supongamos el siguiente texto:
>
> ```
> "Microsoft anunci√≥ una colaboraci√≥n con Apple Inc."
> ```
>
> En este caso, se podr√≠a optar por no convertir el texto a min√∫sculas para preservar informaci√≥n sobre entidades nombradas. De este modo el texto quedar√≠a sin alteraci√≥n:
>
> ```
> "Microsoft anunci√≥ una colaboraci√≥n con Apple Inc."
> ```

#### T√©cnicas para la conversi√≥n del caso

##### Conversi√≥n a min√∫sculas o may√∫sculas con python "puro"

Python proporciona m√©todos sencillos para convertir texto a min√∫sculas. La funci√≥n `lower()` se utiliza para transformar cada car√°cter alfab√©tico en su equivalente en min√∫sculas, mientras que otros caracteres permanecen inalterados. Por ejemplo:

```python
texto = "¬°Este Producto es Fant√°stico!"
texto_min = texto.lower()
print(texto_min)
# Salida: "¬°este producto es fant√°stico!"
```

Para tareas espec√≠ficas, como generaci√≥n de encabezados o formateo de texto en documentos oficiales, se puede utilizar la funci√≥n `upper()` para transformar todo el texto a may√∫sculas. Por ejemplo:

```python
texto = "¬°Este Producto es Fant√°stico!"
texto_may = texto.upper()
print(texto_may)
# Salida: "¬°ESTE PRODUCTO ES FANT√ÅSTICO!"
```

Adem√°s de convertir todo el texto a min√∫sculas o may√∫sculas, es posible utilizar m√©todos como `capitalize()` para convertir solo la primera letra de una oraci√≥n en may√∫scula, o `title()` para capitalizar cada palabra. Esto es √∫til en tareas de formateo. Por ejemplo:

```python
texto = "¬°este producto es fant√°stico!"
texto_capitalizado = texto.capitalize()
print(texto_capitalizado)
# Salida: "¬°este producto es fant√°stico!"

texto_titulo = texto.title()
print(texto_titulo)
# Salida: "¬°Este Producto Es Fant√°stico!"
```

##### Uso de bibliotecas especializadas en Python

Aunque las funciones integradas de Python son √∫tiles, bibliotecas como **nltk** y **spaCy** ofrecen herramientas adicionales para manejar la conversi√≥n del caso de manera m√°s robusta en corpus grandes o multiling√ºes.

Por su lado, **NLTK** aunque no incluye funciones espec√≠ficas para cambiar el caso, su compatibilidad con procesamiento de texto tokenizado permite integrar f√°cilmente la conversi√≥n del caso con otras etapas de limpieza.

Podemos verlo en el siguiente ejemplo:

```python
from nltk.tokenize import word_tokenize
import nltk
nltk.download('punkt')

texto = "¬°Este Producto es Fant√°stico!"
tokens = word_tokenize(texto.lower())
print(tokens)
# Salida: ['¬°', 'este', 'producto', 'es', 'fant√°stico', '!']
```

Por otro lado, **spaCy** ofrece una forma eficiente de procesar texto mientras mantiene el control sobre el caso. Permite aplicar conversiones en palabras individuales mientras se conserva informaci√≥n estructural.

Por ejemplo:

```python
import spacy

nlp = spacy.load("es_core_news_sm")
doc = nlp("¬°Este Producto es Fant√°stico!")
texto_min = " ".join([token.text.lower() for token in doc])
print(texto_min)
# Salida: "¬°este producto es fant√°stico!"
```

> [!note]
>
> La conversi√≥n del caso es un paso sencillo pero fundamental en el preprocesamiento del texto. Aunque a menudo la conversi√≥n a min√∫sculas es suficiente para garantizar la consistencia y reducir redundancias, la decisi√≥n de aplicar esta t√©cnica debe basarse en el contexto de la tarea. Por ejemplo, en an√°lisis de sentimientos, las may√∫sculas suelen ser irrelevantes, mientras que en la extracci√≥n de entidades nombradas o textos donde las siglas son importantes, conservar el caso original puede ser cr√≠tico. Herramientas como Python, nltk y spaCy permiten implementar estas conversiones de manera eficiente y adaptada a las necesidades del proyecto, garantizando resultados consistentes y precisos.

### Eliminaci√≥n de espacios innecesarios: Limpieza para la consistencia del texto

La **eliminaci√≥n de espacios innecesarios** es otro paso importante en la limpieza de texto en proyectos de procesamiento del lenguaje natural. Aunque puede parecer un detalle menor, los espacios redundantes o mal colocados pueden generar inconsistencias en los datos, lo que afecta tanto la representaci√≥n del texto como los resultados de los modelos. As√≠, esta fase tiene como objetivo normalizar la estructura del texto, asegurando que las palabras y frases est√©n correctamente delimitadas por un √∫nico espacio, sin interferencias de caracteres invisibles como tabulaciones o saltos de l√≠nea innecesarios.

En muchas tareas de NLP, los espacios adicionales pueden introducir ruido. Por ejemplo, palabras separadas por m√∫ltiples espacios podr√≠an ser interpretadas de manera err√≥nea por un modelo, afectando la tokenizaci√≥n o el an√°lisis de patrones. Adem√°s, en textos extra√≠dos de fuentes diversas como formularios, bases de datos o archivos PDF, los espacios innecesarios suelen ser un problema recurrente debido al formato del documento o a errores en la extracci√≥n del texto.

Sin embargo, la eliminaci√≥n de espacios tambi√©n debe realizarse con cuidado en contextos donde la ubicaci√≥n del espacio tiene un significado sem√°ntico o estructural, como en formatos tabulares o ciertas expresiones matem√°ticas.

#### T√©cnicas para eliminar espacios innecesarios

##### Uso de m√©todos b√°sicos de Python

Python proporciona m√©todos integrados simples para manejar espacios en el texto. La funci√≥n `strip()` elimina espacios al inicio y al final de una cadena, mientras que `split()` y `join()` pueden usarse en conjunto para eliminar m√∫ltiples espacios entre palabras.

Podemos ver un ejemplo ilustrativo

```python
texto = "  Me  encanta  este   producto.  "

# Eliminar espacios al inicio y al final
texto_limpio = texto.strip()

# Reducir m√∫ltiples espacios a uno solo
texto_limpio = " ".join(texto_limpio.split())

print(texto_limpio)
# Salida: "Me encanta este producto."
```

##### Uso de expresiones regulares

Para un control m√°s preciso, las expresiones regulares con la biblioteca `re` permiten identificar y reemplazar patrones de espacios redundantes. Por ejemplo:

```python
import re

texto = "  Me  encanta  este   producto.  "

# Eliminar espacios al inicio y al final
texto = texto.strip()

# Reemplazar m√∫ltiples espacios con un solo espacio
texto_limpio = re.sub(r'\s+', ' ', texto)

print(texto_limpio)
# Salida: "Me encanta este producto."
```

##### Uso de bibliotecas especializadas como `nltk` y `spaCy`

Bibliotecas como **nltk** y **spaCy** no ofrecen directamente funciones espec√≠ficas para la eliminaci√≥n de espacios, pero su integraci√≥n en el preprocesamiento facilita el manejo de texto limpio y tokenizado.

Veamos un par de ejemplos: 

```python
from nltk.tokenize import word_tokenize
import nltk
nltk.download('punkt')

texto = "  Me  encanta  este   producto.  "

# Tokenizaci√≥n con eliminaci√≥n de espacios adicionales
tokens = word_tokenize(texto)
texto_limpio = " ".join(tokens)

print(texto_limpio)
# Salida: "Me encanta este producto ."
```

```python
import spacy

nlp = spacy.load("es_core_news_sm")
texto = "  Me  encanta  este   producto.  "

# Procesar texto con spaCy
doc = nlp(texto.strip())
texto_limpio = " ".join([token.text for token in doc])

print(texto_limpio)
# Salida: "Me encanta este producto ."
```

> [!note]
>
> La eliminaci√≥n de espacios innecesarios es un paso sencillo pero crucial en la limpieza del texto. Este proceso asegura que los datos sean consistentes y libres de ruido, lo que facilita la tokenizaci√≥n y el an√°lisis posterior. Sin embargo, debe adaptarse al contexto de la tarea. En casos generales, eliminar espacios adicionales mejora la calidad del texto; sin embargo, en situaciones como la manipulaci√≥n de datos tabulares, se debe proceder con cuidado para no alterar estructuras importantes. Python, con sus herramientas integradas y bibliotecas especializadas como `nltk` y `spaCy`, ofrece m√∫ltiples enfoques para implementar esta t√©cnica de manera eficiente y precisa.

### **Correcci√≥n ortogr√°fica en textos: cu√°ndo y c√≥mo implementarla**

La correcci√≥n ortogr√°fica es un paso opcional pero relevante en el preprocesamiento de texto. Aunque no siempre es necesario, este proceso puede marcar la diferencia en tareas donde los errores tipogr√°ficos o de escritura afectan la calidad del an√°lisis. En este apartado exploraremos las razones para realizar una correcci√≥n ortogr√°fica, las herramientas disponibles y c√≥mo implementarlas de manera pr√°ctica.

#### La importancia de la correcci√≥n ortogr√°fica

En proyectos de procesamiento del lenguaje natural, los errores ortogr√°ficos pueden introducir ruido al texto, dificultando la interpretaci√≥n del mismo. Por ejemplo, palabras mal escritas como "incorecto", en vez de "incorrecto" podr√≠an ser tratadas como t√©rminos diferentes, aumentando la dimensionalidad del vocabulario y afectando el rendimiento de los modelos.

La correcci√≥n ortogr√°fica es particularmente √∫til en tareas donde la calidad del texto tiene un impacto directo, como:

- **An√°lisis de texto informal**: En redes sociales o chats, donde los errores ortogr√°ficos son comunes.
- **Sistemas de b√∫squeda**: Al corregir palabras clave con errores, se mejora la precisi√≥n de las consultas.
- **Procesamiento de datos hist√≥ricos o escaneados**: Los textos extra√≠dos con OCR suelen contener errores que deben corregirse.

Sin embargo, no siempre es recomendable aplicar correcci√≥n ortogr√°fica. En algunos casos, los errores forman parte del contexto, como en el an√°lisis de lenguaje informal, donde una palabra mal escrita puede transmitir un significado espec√≠fico.

#### T√©cnicas de correcci√≥n ortogr√°fica

Existen diferentes enfoques para corregir errores ortogr√°ficos, desde m√©todos manuales con diccionarios hasta herramientas especializadas dise√±adas para manejar grandes vol√∫menes de texto.

##### Uso de un diccionario manual

Una de las formas m√°s simples de realizar correcciones ortogr√°ficas es mediante un diccionario de palabras v√°lidas. Este enfoque es adecuado para textos peque√±os o en dominios espec√≠ficos donde se pueda definir un conjunto de palabras relevantes.

Por ejemplo, sup√≥n que tenemos un texto con varios errores ortogr√°ficos. Podr√≠amos usar un diccionario manual como el del c√≥digo a continuaci√≥n para corregirlos

```python
# Diccionario de palabras v√°lidas
diccionario = {
  "holaa": "hola",
  "m√∫ndo": "mundo",
  "esste": "este",
  "textoo": "texto",
  "incorecto": "incorrecto"
}

# Texto con errores ortogr√°ficos
texto = "Holaa m√∫ndo! Esste textoo contiene palabras incorecto escritas."

# Corregimos las palabras usando el diccionario
palabras = texto.lower().split()
texto_corregido = " ".join([diccionario.get(palabra, palabra) for palabra in palabras])

print("Texto corregido manualmente:", texto_corregido)
```

En este caso, el texto original es transformado en un texto limpio y corregido. Aunque este m√©todo es f√°cil de implementar, se vuelve muy poco pr√°ctico en textos grandes o multiling√ºes.

##### Correcci√≥n ortogr√°fica con TextBlob

Cuando el volumen del texto es mayor o se busca automatizaci√≥n, herramientas como **TextBlob** son una excelente alternativa. TextBlob utiliza un enfoque basado en probabilidades para detectar y corregir errores comunes de escritura.

Veamos en acci√≥n esta biblioteca en el siguiente ejemplo

```python
from textblob import TextBlob

# Texto con errores ortogr√°ficos
texto = "Holaa m√∫ndo! Esste textoo contiene palabras increctamente escritas."

# Crear un objeto TextBlob
blob = TextBlob(texto)

# Corregir el texto
texto_corregido = blob.correct()

print("Texto corregido con TextBlob:", texto_corregido)
```

La salida esperada ser√≠a 

```plaintext
Texto corregido con TextBlob: Hola mundo! Este texto contiene palabras incorrectamente escritas.
```

Aunque TextBlob es sencillo de implementar y eficaz, est√° dise√±ado principalmente para ingl√©s. En espa√±ol, los resultados pueden variar dependiendo de la complejidad del texto.

##### Correcci√≥n ortogr√°fica con SymSpell

Si el texto es extenso o requiere una correcci√≥n altamente eficiente, **SymSpell** es una de las mejores herramientas disponibles. SymSpell utiliza el algoritmo de distancia de edici√≥n (Levenshtein) para identificar errores y sugerir correcciones basadas en un diccionario predefinido.

El siguiente c√≥digo muestra un ejemplo pr√°ctico

```python
from symspellpy import SymSpell, Verbosity

# Inicializamos SymSpell
sym_spell = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)

# Cargar un diccionario predefinido (puede ser un archivo de palabras en espa√±ol)
dictionary_path = "frequency_dictionary_en_82_765.txt" # Cambiar por un diccionario en espa√±ol
sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)

# Texto con errores ortogr√°ficos
texto = "Holaa m√∫ndo! Esste textoo contiene palabras incorectamente escritas."

# Tokenizar y corregir cada palabra
tokens = texto.lower().split()
correcciones = [sym_spell.lookup(token, Verbosity.CLOSEST, max_edit_distance=2) for token in tokens]

# Convertir las correcciones a texto limpio
texto_corregido = " ".join([correc[0].term if correc else token for token, correc in zip(tokens, correcciones)])

print("Texto corregido con SymSpell:", texto_corregido)
```

SymSpell es extremadamente r√°pido y permite personalizar los diccionarios seg√∫n el idioma o dominio. Sin embargo, requiere una configuraci√≥n previa ya que necesita la carga de un diccionario adecuado.

#### Comparaci√≥n de herramientas y enfoques

Cada t√©cnica tiene sus ventajas y limitaciones. La elecci√≥n depender√° del contexto, el idioma y la escala del proyecto.

| **T√©cnica**    | **Ventajas**            | **Limitaciones**               |
| ------------------ | ----------------------------------- | --------------------------------------------- |
| Diccionario manual | Control total, f√°cil de implementar | Poco pr√°ctico para textos grandes o complejos |
| TextBlob      | Sencillo, corrige autom√°ticamente  | Soporte limitado para espa√±ol         |
| SymSpell      | R√°pido y escalable         | Requiere configuraci√≥n y diccionario adecuado |

> [!Note]
>
> Aunque √∫til, la correcci√≥n ortogr√°fica no siempre es necesaria. En tareas como el an√°lisis de lenguaje informal, donde los errores son parte del contexto, aplicar correcciones podr√≠a eliminar se√±ales importantes. Sin embargo, en dominios t√©cnicos o formales, esta t√©cnica puede mejorar significativamente los resultados.
>
> La correcci√≥n ortogr√°fica es particularmente valiosa en sistemas de b√∫squeda, procesamiento de datos OCR y clasificaci√≥n de texto en dominios donde los errores tipogr√°ficos afectan la calidad del an√°lisis.
>
> Con herramientas como TextBlob y SymSpell, es posible implementar correcciones autom√°ticas de forma r√°pida y eficiente, adapt√°ndolas a las necesidades del proyecto.

### Eliminaci√≥n de stop-words

La **eliminaci√≥n de  stop-words** es una fase importante en el preprocesamiento de texto. Este proceso consiste en identificar y eliminar palabras que aparecen con alta frecuencia en el texto, pero que tienen poca o nula carga sem√°ntica. Estas palabras incluyen art√≠culos, preposiciones, pronombres y conjunciones, como "el", "de", "y", "que", "en", "por", entre otras.

El prop√≥sito de eliminar estas palabras es reducir el tama√±o del vocabulario y centrarse en los t√©rminos que realmente aportan informaci√≥n valiosa para la tarea en cuesti√≥n. Sin embargo, es importante tener en cuenta que en ciertas tareas, como el an√°lisis gramatical o la identificaci√≥n de relaciones entre palabras, las  stop-words pueden ser necesarias.

> [!note] 
> Es importante recordar que no siempre es necesario eliminar las  stop-words. Por ejemplo, en tareas como la identificaci√≥n de entidades nombradas (NER) o el an√°lisis de lenguaje informal, pueden ser √∫tiles para mantener el contexto completo del texto.

As√≠, se puede afirmar sin lugar a dudas que la eliminaci√≥n de stop-words tiene m√∫ltiples beneficios y su impacto es clave para mejorar el rendimiento de los modelos y simplificar el an√°lisis textual. Aparte de de reducir el tama√±o del vocabulario, existen otras razones para hacer un buen procesado de ellas. Veamos las m√°s importantes

#### Reducci√≥n de dimensionalidad

Cada palabra en un texto representa una dimensi√≥n en el espacio vectorial que los modelos de aprendizaje autom√°tico deben procesar. Eliminar palabras irrelevantes como las  stop-words reduce significativamente el tama√±o del vocabulario, lo que, a su vez, disminuye la complejidad computacional. Esto es especialmente importante cuando trabajamos con grandes corpus de texto, donde el vocabulario puede llegar a contener decenas de miles de palabras. Si eliminamos estas palabras frecuentes pero in√∫tiles, el modelo podr√° enfocarse en las dimensiones que realmente aportan informaci√≥n valiosa.

#### Eliminaci√≥n de ruido

Las  stop-words son, en su mayor√≠a, palabras funcionales que no a√±aden significado contextual espec√≠fico. En tareas como el an√°lisis de sentimientos, clasificaci√≥n de texto o generaci√≥n de res√∫menes, su presencia puede introducir ruido en el an√°lisis, dificultando que el modelo identifique patrones √∫tiles. Al eliminarlas, el texto se vuelve m√°s claro y los modelos pueden concentrarse en las palabras clave que realmente capturan el significado del contenido.

#### Enfoque en palabras clave

Cuando eliminamos las  stop-words, lo que logramos es dar prioridad a las palabras con mayor carga sem√°ntica. Por ejemplo, en un texto como:

> *"El preprocesamiento de texto incluye eliminar las palabras vac√≠as."*

las palabras verdaderamente relevantes son **preprocesamiento**, **texto**, **eliminar** y **vac√≠as**. Estas palabras contienen el n√∫cleo del mensaje, mientras que t√©rminos como "el", "de" e "incluye" solo sirven como conectores que no a√±aden valor contextual. La eliminaci√≥n de stop-words nos permite mantener un enfoque limpio y significativo en el an√°lisis.

#### Ajustes para tareas espec√≠ficas

Es importante notar que **no siempre conviene eliminar todas las  stop-words**. En algunos contextos, estas palabras pueden ser necesarias. Por ejemplo, en tareas como el an√°lisis de sentimientos, la palabra "no" es fundamental porque indica una negaci√≥n que puede cambiar completamente el significado de una frase. Consideremos los ejemplos:

> 1. "Este producto es excelente."
> 2. "Este producto no es excelente."

Eliminar la palabra "no" en este caso llevar√≠a a interpretaciones completamente err√≥neas. Por lo tanto, el uso de listas personalizadas de  stop-words, ajustadas a la tarea y al dominio del problema, puede ser m√°s adecuado que usar listas predefinidas.

#### Eliminaci√≥n de  stop-words usando bibliotecas NLP en python

A continuaci√≥n, exploraremos c√≥mo implementar la eliminaci√≥n de  stop-words utilizando dos de las bibliotecas m√°s populares para tareas de NLP: **NLTK** y **spaCy**. Ambos enfoques tienen ventajas particulares y se adaptan a diferentes necesidades.

##### NLTK

Como ya hemos visto, NLTK (Natural Language Toolkit) es una biblioteca cl√°sica para procesamiento de lenguaje natural que incluye una lista predefinida de stop-words para varios idiomas, incluyendo espa√±ol. Aunque es menos eficiente que otras bibliotecas modernas como spaCy, NLTK es ampliamente utilizado en proyectos educativos y an√°lisis b√°sicos por su simplicidad y flexibilidad.

Eliminar  stop-words utilizando **NLTK** es un proceso bastante directo y f√°cil de implementar. El procedimiento se basa en tres pasos fundamentales: La carga de las stop-words, la *tokenizaci√≥n* y el filtrado del texto para quedarnos √∫nicamente con las palabras realmente relevantes. Exploremos cada paso con detalle.

###### Cargar las stop-words del idioma correspondiente

El primer paso consiste en obtener una lista de  stop-words que corresponda al idioma del texto que queremos procesar. NLTK incluye listas predefinidas de  stop-words para diferentes idiomas, y en nuestro caso, seleccionaremos las palabras en espa√±ol. Estas listas contienen t√©rminos como "el", "de", "la", "en", "por", entre otros, que suelen aparecer con mucha frecuencia en los textos, pero que rara vez aportan informaci√≥n √∫til.

Para cargar esta lista, basta con utilizar el m√≥dulo `stopwords` de NLTK. Este m√≥dulo nos proporciona una forma sencilla de acceder a las palabras irrelevantes del idioma que necesitemos.

```python
from nltk.corpus import stopwords

# Descargar las  stop-words en espa√±ol
nltk.download('stopwords')

# Obtener las  stop-words en espa√±ol
stop_words = set(stopwords.words('spanish'))
```

Con este paso inicial, ya tenemos una lista completa de palabras comunes en espa√±ol que NLTK considera  stop-words. Es importante destacar que, si necesitamos personalizar esta lista a√±adiendo o eliminando palabras seg√∫n los requerimientos de nuestra tarea, tambi√©n podemos hacerlo f√°cilmente.

###### Tokenizar el texto para separar las palabras

Una vez que tenemos nuestras  stop-words cargadas, el siguiente paso es dividir el texto en palabras individuales, un proceso conocido como **tokenizaci√≥n**. Esta etapa es crucial porque necesitamos analizar cada palabra de forma independiente para verificar si est√° presente en la lista de  stop-words.

Para tokenizar el texto, NLTK ofrece una funci√≥n llamada `word_tokenize`. Esta herramienta separa el texto en tokens, que suelen ser palabras, aunque tambi√©n puede incluir signos de puntuaci√≥n u otros caracteres seg√∫n el texto original.

Por ejemplo, si tenemos el texto:

> *"El preprocesamiento de texto incluye eliminar las palabras vac√≠as o  stop-words."*

 al aplicarle `word_tokenize`, obtendremos una lista como esta:

```python
from nltk.tokenize import word_tokenize

# Tokenizar el texto
texto = "El preprocesamiento de texto incluye eliminar las palabras vac√≠as o  stop-words."
tokens = word_tokenize(texto)

print(tokens)
# Salida: ['El', 'preprocesamiento', 'de', 'texto', 'incluye', 'eliminar', 'las', 'palabras', 'vac√≠as', 'o', 'stop', 'words', '.']
```

La tokenizaci√≥n nos permite trabajar con el texto palabra por palabra, facilitando la identificaci√≥n de las  stop-words y preparando el terreno para el siguiente paso.

###### Filtrar las palabras que no est√°n en la lista de  stop-words

Finalmente, con la lista de tokens en mano, podemos realizar el filtrado. Este paso consiste en recorrer cada palabra tokenizada y comprobar si pertenece a la lista de  stop-words. Si la palabra est√° en la lista, la descartamos. Si no est√°, la conservamos. El resultado ser√° un conjunto de palabras que tienen mayor relevancia para nuestra tarea.

El filtrado se puede implementar de manera sencilla utilizando una comprensi√≥n de listas en Python. Por ejemplo:

```python
# Filtrar las palabras que no est√°n en la lista de  stop-words
palabras_filtradas = [palabra for palabra in tokens if palabra.lower() not in stop_words]

print(palabras_filtradas)
# Salida: ['preprocesamiento', 'texto', 'incluye', 'eliminar', 'palabras', 'vac√≠as', 'stop', 'words']
```

En este caso, palabras como "El", "de", "las" y "o" han sido eliminadas porque est√°n incluidas en la lista de  stop-words, mientras que t√©rminos m√°s significativos como "preprocesamiento", "texto" y "vac√≠as" se han conservado. Observa que utilizamos el m√©todo `lower()` para convertir las palabras a min√∫sculas, garantizando que la comparaci√≥n con las  stop-words sea insensible al caso.

> [!Note]
>
> Al completar estos pasos, se logra transformar un texto cargado de ruido en un conjunto m√°s limpio y enfocado de palabras clave. Este enfoque b√°sico es suficiente para muchas tareas de NLP y es una excelente manera de preparar los datos antes de avanzar a etapas m√°s complejas como la codificaci√≥n del texto.
>
> No obstante, recuerda que este proceso no es universal. En algunos proyectos, puede ser necesario modificar la lista de  stop-words seg√∫n los objetivos del an√°lisis. Por ejemplo, si est√°s trabajando con an√°lisis de sentimientos, podr√≠as decidir conservar palabras como "no", ya que estas pueden cambiar radicalmente el significado de una oraci√≥n.
>
> En resumen, la eliminaci√≥n de  stop-words con NLTK es una herramienta poderosa y flexible, ideal para dar los primeros pasos en la limpieza y preprocesamiento de textos. Aunque sencilla, esta t√©cnica tiene un impacto significativo en la calidad de los datos y, por ende, en el rendimiento de los modelos que se construyan posteriormente.

##### spaCy

Eliminar  stop-words utilizando **spaCy** es un proceso aun m√°s √°gil, que combina eficiencia y simplicidad. Gracias a sus modelos preentrenados, spaCy permite identificar r√°pidamente las  stop-words y realizar el filtrado de palabras en un texto con muy pocas l√≠neas de c√≥digo. A continuaci√≥n, detallamos el procedimiento paso a paso para realizar esta tarea, desde la carga del modelo hasta la eliminaci√≥n de las palabras irrelevantes.

###### Cargar el modelo ling√º√≠stico para el idioma correspondiente

El primer paso para trabajar con spaCy es cargar un modelo ling√º√≠stico que est√© adaptado al idioma del texto que queremos procesar. SpaCy ofrece modelos preentrenados para varios idiomas, y para textos en espa√±ol, utilizaremos el modelo `es_core_news_sm`, correspondiente al conjunto m√°s peque√±o y manejable. Este modelo contiene no solo una lista predefinida de  stop-words, sino tambi√©n capacidades adicionales como an√°lisis gramatical, lematizaci√≥n y reconocimiento de entidades nombradas, lo que lo hace muy completo.

Cargar el modelo es tan sencillo como usar la funci√≥n `spacy.load`. Una vez cargado, el modelo estar√° listo para procesar textos y analizar todas sus caracter√≠sticas ling√º√≠sticas:

```python
import spacy

# Cargar el modelo en espa√±ol
nlp = spacy.load("es_core_news_sm")
```

Al realizar este paso, spaCy carga todos los recursos necesarios para trabajar con textos en espa√±ol, incluyendo una lista completa de  stop-words. Es importante mencionar que esta lista es personalizable, lo que significa que podemos a√±adir o eliminar palabras seg√∫n las necesidades de nuestro proyecto.

###### Procesar el texto para dividirlo en tokens

Despu√©s de cargar el modelo, el siguiente paso es procesar el texto con spaCy para dividirlo en **tokens**. En spaCy, al procesar un texto con el modelo ling√º√≠stico `nlp()`, se genera un objeto de tipo`doc`. Este objeto contiene una representaci√≥n estructurada del texto, donde cada palabra, signo de puntuaci√≥n y s√≠mbolo es tratado como un token individual.

Por ejemplo, supongamos que queremos procesar el texto:

> *"El preprocesamiento de texto incluye eliminar las palabras vac√≠as o  stop-words."*

Al procesarlo con spaCy, obtenemos una lista de tokens que conserva tanto las palabras como los signos de puntuaci√≥n:

```python
# Texto de ejemplo
texto = "El preprocesamiento de texto incluye eliminar las palabras vac√≠as o  stop-words."

# Procesar el texto
doc = nlp(texto)

# Visualizar los tokens
tokens = [token.text for token in doc]
print(tokens)
# Salida: ['El', 'preprocesamiento', 'de', 'texto', 'incluye', 'eliminar', 'las', 'palabras', 'vac√≠as', 'o', 'stop', 'words', '.']
```

En este punto, el texto ya ha sido tokenizado y podemos acceder a cada palabra de manera individual. Lo interesante es que spaCy no solo separa las palabras, sino que tambi√©n asigna a cada token propiedades como su parte del discurso (POS), si es una stop word, su forma base (lema), entre otras.

###### Eliminar los tokens que est√©n marcados como  stop-words

Una vez que tenemos el texto tokenizado, llega el momento de filtrar las ** stop-words**. Una de las ventajas de spaCy es que cada token tiene un atributo llamado `is_stop`, que indica si esa palabra est√° considerada como una stop word en el modelo. Con esta propiedad, podemos filtrar f√°cilmente los tokens irrelevantes y conservar solo aquellos que aportan valor al an√°lisis.

Por ejemplo, para el texto mencionado anteriormente, el filtrado se har√≠a de la siguiente manera:

```python
# Filtrar las palabras que no son  stop-words
palabras_filtradas = [token.text for token in doc if not token.is_stop]

print(palabras_filtradas)
# Salida: ['preprocesamiento', 'texto', 'incluye', 'eliminar', 'palabras', 'vac√≠as', 'stop', 'words']
```

Aqu√≠ podemos observar c√≥mo palabras como "El", "de", "las" y "o" han sido eliminadas, mientras que las palabras clave como "preprocesamiento", "texto" y "vac√≠as" se han conservado. Esto nos deja con un texto mucho m√°s limpio y relevante.

> [!Note]
>
> El proceso de eliminaci√≥n de  stop-words con spaCy es notablemente eficiente y est√° dise√±ado para integrarse perfectamente con otras tareas de procesamiento de texto. Desde el momento en que cargamos el modelo ling√º√≠stico, spaCy nos proporciona herramientas avanzadas para tokenizar el texto y clasificar cada palabra seg√∫n su relevancia. Esto no solo facilita la eliminaci√≥n de  stop-words, sino que tambi√©n nos abre la puerta a realizar an√°lisis m√°s complejos, como lematizaci√≥n o reconocimiento de entidades nombradas. Su capacidad para manejar grandes vol√∫menes de texto y ofrecer informaci√≥n contextual hace que sea una herramienta ideal para proyectos en los que se demanda eficiencia y precisi√≥n en el preprocesamiento del lenguaje.



> ###### C√≥mo personalizar las  stop-words de spaCy
>
> Una de las grandes ventajas de trabajar con **spaCy** es la posibilidad de personalizar su lista de ** stop-words** para adaptarla a las necesidades espec√≠ficas de tu proyecto. Aunque spaCy proporciona una lista predefinida de palabras irrelevantes para cada idioma, es com√∫n que necesitemos a√±adir o eliminar t√©rminos seg√∫n el contexto. Por ejemplo, en un an√°lisis t√©cnico, podr√≠as decidir que palabras como "datos" o "informaci√≥n", que suelen aparecer con frecuencia, no aportan valor y deber√≠an tratarse como  stop-words. Por otro lado, podr√≠as querer conservar palabras que spaCy considera irrelevantes, pero que son importantes para tu tarea, como "no" en un an√°lisis de sentimientos.
>
> Personalizar las  stop-words en spaCy es un proceso que se realiza directamente sobre su modelo ling√º√≠stico.
>
> **Para incluir una palabra en la lista de  stop-words de spaCy**, solo necesitas acceder al atributo `stop_words` del vocabulario del modelo (`nlp.vocab`) y marcar esa palabra como una stop word. Por ejemplo, si decides que "python" y "spaCy" deben tratarse como  stop-words, el c√≥digo ser√≠a:
>
> ```python
> # A√±adir palabras personalizadas como  stop-words
> nlp.vocab["python"].is_stop = True
> nlp.vocab["spaCy"].is_stop = True
> ```
>
> A partir de este momento, spaCy reconocer√° estas palabras como  stop-words y las marcar√° autom√°ticamente en los textos procesados. Por contra, **si necesitas conservar ciertas palabras que spaCy considera como  stop-words por defecto**, simplemente debes marcarlas como no- stop-words. Por ejemplo, para mantener "no" y "muy" en tu an√°lisis, puedes eliminarlas de la lista de  stop-words con el siguiente c√≥digo:
>
> ```python
> # Eliminar palabras de la lista de  stop-words
> nlp.vocab["no"].is_stop = False
> nlp.vocab["muy"].is_stop = False
> ```
>
> De este modo, estas palabras dejar√°n de ser ignoradas y estar√°n disponibles para el an√°lisis posterior.
>
> Si deseas **consultar o revisar la lista completa de  stop-words** que spaCy utiliza en tu modelo, puedes hacerlo accediendo al atributo `nlp.Defaults.stop_words`. Esto puede ser √∫til para asegurarte de que la personalizaci√≥n de tu lista se alinea con los objetivos del proyecto:
>
> ```python
> # Ver la lista completa de  stop-words
> print(nlp.Defaults.stop_words)
> ```

##### Comparaci√≥n entre NLTK y spaCy para eliminaci√≥n de  stop-words

La siguiente tabla hace un resumen comparativo del funcionamiento de NLTK y spaCy a la hora de procesar stop-wors

| **Criterio**      | **NLTK**                | **spaCy**               |
| ---------------------- | --------------------------------------- | ------------------------------------- |
| **Velocidad**     | Menor velocidad en corpus grandes.   | Alta eficiencia en textos extensos.  |
| **Facilidad de uso**  | Requiere m√°s configuraci√≥n inicial.   | M√°s sencillo y r√°pido de implementar. |
| **Idiomas soportados** | Amplio soporte, pero limitado en lemas. | Modelos espec√≠ficos por idioma.    |
| **Manejo de contexto** | Simple eliminaci√≥n de palabras.     | Integraci√≥n con an√°lisis sint√°ctico. |

> [!Tip] 
> Si trabajas con corpus grandes y necesitas un procesamiento m√°s eficiente, **spaCy** es una mejor opci√≥n. Para proyectos m√°s peque√±os o educativos, **NLTK** ofrece un enfoque flexible y f√°cil de aprender.

#### Buenas pr√°cticas para eliminar  stop-words

Finalmente es interesante tener claras unas buenas pr√°cticas a la hora de trabajar con  stop-words, que podr√≠an reducirse a lo siguiente

##### Personalizaci√≥n de la lista de  stop-words

Aunque las bibliotecas incluyen listas predefinidas, es recomendable adaptarlas a la tarea. Por ejemplo, palabras como "python" o "datos" podr√≠an eliminarse si aparecen frecuentemente en un corpus t√©cnico.

```python
stop_words.add("python")
stop_words.add("datos")
```

##### Evitar eliminar palabras clave relevantes

En ciertos casos, palabras muy frecuentes pueden ser importantes, como "no" en un an√°lisis de sentimiento. Es importante revisar el impacto de las  stop-words en los resultados.

##### Considerar el idioma del texto

Siempre verifica que las  stop-words correspondan al idioma del texto. Una lista de  stop-words en ingl√©s no ser√° √∫til para textos en espa√±ol.

> [!warning]
>
> La eliminaci√≥n de stop-words es un paso esencial en el preprocesamiento de texto para reducir el ruido y enfocar el an√°lisis en las palabras clave. Herramientas como **NLTK** y **spaCy** ofrecen m√©todos eficientes para realizar este proceso, pero es importante ajustar las listas de  stop-words seg√∫n los objetivos espec√≠ficos del proyecto. Con una correcta implementaci√≥n, este paso mejora la calidad de los datos y facilita la representaci√≥n num√©rica del texto.

¬°Entendido! Vamos a ajustar el estilo para que sea m√°s narrativo, eliminando las enumeraciones y presentando la informaci√≥n de forma m√°s fluida y descriptiva. Aqu√≠ tienes la versi√≥n revisada:

------

### Tokenizaci√≥n de textos

La tokenizaci√≥n es una fase crucial en el preprocesamiento de textos dentro de las tareas de procesamiento de lenguaje natural. B√°sicamente, este proceso consiste en dividir un texto en unidades m√°s peque√±as, conocidas como ***tokens***. Estas unidades pueden variar seg√∫n la granularidad que se elija, pudiendo ser palabras, subpalabras, caracteres o incluso oraciones completas.

Dividir el texto en tokens facilita que los modelos de NLP puedan trabajar de manera eficiente con el contenido textual. Sin embargo, aunque el concepto puede parecer simple, realizar una tokenizaci√≥n efectiva no siempre es trivial. Hay que tener en cuenta diversos factores, como las caracter√≠sticas del idioma en el que est√° escrito el texto, el manejo de la puntuaci√≥n y la normalizaci√≥n. Cada detalle puede influir significativamente en el rendimiento de los modelos, lo que subraya la importancia de esta etapa.

El caso m√°s habitual de tokenizaci√≥n es el basado en palabras. Por ejemplo, si tomamos una frase como *"¬°Hola mundo! ¬øC√≥mo est√°s?"*, la tokenizaci√≥n dividir√≠a este texto en unidades como *["¬°Hola", "mundo", "!", "¬øC√≥mo", "est√°s", "?"]*. Sin embargo, en otras aplicaciones podr√≠a ser m√°s √∫til dividir el texto a nivel de caracteres, obteniendo *["¬°", "H", "o", "l", "a", " ", "m", "u", "n", "d", "o", "!"]*. En el caso de los modelos modernos, como los basados en transformers, es com√∫n usar tokenizaci√≥n de subpalabras. Esto permite dividir palabras infrecuentes o desconocidas en fragmentos m√°s peque√±os que pueden ser representados f√°cilmente en un vocabulario fijo. Por ejemplo, la palabra *"correremos"* podr√≠a dividirse en las subpalabras *["corr", "eremos"]*. Tambi√©n existe la opci√≥n de tokenizar por oraciones, lo que puede ser √∫til en tareas como el an√°lisis de sentimientos a nivel oraci√≥n o la generaci√≥n de res√∫menes.

El idioma juega un papel fundamental. En espa√±ol e ingl√©s, las palabras suelen estar separadas por espacios, lo que simplifica el proceso. Sin embargo, en lenguajes como el chino o japon√©s, no existen delimitadores claros entre palabras, lo que hace que la tokenizaci√≥n dependa de modelos espec√≠ficos que puedan identificar los l√≠mites. Otro aspecto importante es el manejo de la puntuaci√≥n. En algunos casos, como el an√°lisis sint√°ctico, conservar los signos de puntuaci√≥n puede ser relevante, mientras que en otros, como la clasificaci√≥n de texto, podr√≠an considerarse ruido. Adem√°s, existen retos como las palabras compuestas, los ap√≥strofes y las contracciones. Por ejemplo, *"it's"* podr√≠a expandirse a *"it is"* dependiendo de las necesidades de la tarea.

Cuando se trabaja con vocabularios cerrados, tambi√©n es importante considerar c√≥mo se manejan los tokens que no est√°n presentes en el vocabulario, conocidos como tokens fuera de vocabulario (OOV, por sus siglas en ingl√©s). Los enfoques modernos, como el uso de subpalabras mediante t√©cnicas como Byte Pair Encoding (BPE) o WordPiece, son particularmente √∫tiles para mitigar este problema. Estas t√©cnicas permiten representar palabras raras como combinaciones de fragmentos m√°s comunes.

En t√©rminos pr√°cticos, pueden usarse diversas herramientas para llevar a cabo la tokenizaci√≥n. En este tema, como en otras secciones, exploraremos como lo hacen NLTK y spaCy. Como ya hemos visto repetidamente, ambas son ampliamente utilizadas en tareas de NLP, aunque cada una tiene caracter√≠sticas que las hacen m√°s adecuadas seg√∫n para qu√© prop√≥sitos.

#### NLTK

Realizar una tokenizaci√≥n b√°sica en NLTK es bastante sencillo. Por ejemplo, para dividir un texto en palabras, basta con utilizar la funci√≥n `word_tokenize`, la cual separa autom√°ticamente los tokens respetando la puntuaci√≥n y los espacios. Si en lugar de palabras se desea tokenizar el texto en oraciones, NLTK tambi√©n incluye la funci√≥n `sent_tokenize`, que identifica los l√≠mites de cada oraci√≥n. Aunque NLTK es muy poderosa, es importante mencionar que, al ser m√°s modular y acad√©mica, puede requerir configuraciones adicionales cuando se trabaja con idiomas distintos al ingl√©s.

Veamos un ejemplo pr√°ctico de tokenizaci√≥n con NLTK

```python
# Importamos la biblioteca
import nltk
from nltk.tokenize import word_tokenize, sent_tokenize

# Descargamos el modelo de tokenizaci√≥n Punkt
nltk.download('punkt')

# Texto de ejemplo
texto = "¬°Hola mundo! ¬øC√≥mo est√°s? Hoy es un gran d√≠a para aprender NLP."

# Tokenizaci√≥n en palabras
tokens_palabras = word_tokenize(texto)
print("Tokens por palabras:", tokens_palabras)

# Tokenizaci√≥n en oraciones
tokens_oraciones = sent_tokenize(texto)
print("Tokens por oraciones:", tokens_oraciones)
```



#### SpaCy

Para tokenizar texto, por ejemplo en espa√±ol, solo necesitas cargar el modelo correspondiente. Ya hemos usado `es_core_news_sm`, y, una vez realizada la carga, basta procesar el texto con la clase `nlp` y un objeto tipo `doc`. Como ya hemos visto, el resultado ser√° un documento que ya incluye informaci√≥n estructurada sobre los tokens, las oraciones o el tipo gramatical de cada palabra entre otra informaci√≥n. Una ventaja significativa de spaCy es su soporte multiling√ºe y su capacidad para manejar autom√°ticamente aspectos como puntuaci√≥n, espacios y palabras compuestas.

Veamos ahora tambi√©n un peque√±o ejemplo de tokenizaci√≥n con spaCy

```python
# Importamos spaCy
import spacy

# Cargamos el modelo en espa√±ol
# Nota: Si no tienes el modelo instalado, usa `!python -m spacy download es_core_news_sm`
nlp = spacy.load("es_core_news_sm")

# Texto de ejemplo
texto = "¬°Hola mundo! ¬øC√≥mo est√°s? Hoy es un gran d√≠a para aprender NLP."

# Procesamos el texto con el modelo de spaCy
doc = nlp(texto)

# Tokenizaci√≥n en palabras
tokens_palabras = [token.text for token in doc]
print("Tokens por palabras:", tokens_palabras)

# Tokenizaci√≥n en oraciones
tokens_oraciones = [sent.text for sent in doc.sents]
print("Tokens por oraciones:", tokens_oraciones)
```

> [!tip]
>
> A pesar de las diferencias entre ambas herramientas, tanto NLTK como spaCy son soluciones poderosas para realizar tokenizaci√≥n. NLTK es ideal para quienes est√°n comenzando a explorar el procesamiento de lenguaje natural o necesitan una biblioteca que les permita realizar experimentos detallados. En cambio, spaCy est√° m√°s orientado a quienes buscan una soluci√≥n robusta para implementaciones pr√°cticas.

### Lematizaci√≥n y stemming en procesamiento de lenguaje natural

Cuando trabajamos con texto en procesamiento de lenguaje natural (NLP), las palabras suelen presentarse en m√∫ltiples formas debido a las variaciones gramaticales. Por ejemplo, en espa√±ol, el verbo "correr" puede aparecer como "corriendo", "corri√≥", "corro", entre otros. Estas variaciones pueden complicar el an√°lisis del texto, ya que modelos y algoritmos suelen tratar cada forma gramatical como una unidad diferente. Es aqu√≠ donde entran en juego la **lematizaci√≥n** y el **stemming**, dos t√©cnicas dise√±adas para reducir las palabras a una forma b√°sica o ra√≠z.

Aunque ambas t√©cnicas buscan simplificar las palabras y reducir la complejidad, hay diferencias importantes entre ellas. Comprender estas diferencias y saber cu√°ndo utilizar cada una es esencial para el √©xito de cualquier proyecto de NLP.

#### ¬øQu√© es la lematizaci√≥n?

La lematizaci√≥n es el proceso de reducir una palabra a su **forma base o lema**. Esta forma base **es una palabra v√°lida en el idioma**, como el infinitivo en el caso de los verbos o el singular en los sustantivos. Por ejemplo, las palabras "corriendo", "corri√≥" y "corro" ser√≠an transformadas en "correr".

Lo que hace especial a la lematizaci√≥n es que utiliza informaci√≥n ling√º√≠stica, como el contexto gramatical y el tipo de palabra (parte del discurso o POS, por sus siglas en ingl√©s), para determinar la forma base. Esto significa que necesita identificar si una palabra es un verbo, un sustantivo o un adjetivo, ya que el lema puede variar seg√∫n la funci√≥n de la palabra en la oraci√≥n. Por ejemplo, en ingl√©s, la palabra "better" puede ser un adjetivo o un verbo, y el lema ser√° diferente en cada caso ("good" si es un adjetivo, "better" si es un verbo).

#### ¬øQu√© es el stemming?

El stemming, por otro lado, es una t√©cnica m√°s sencilla y menos precisa que intenta reducir una palabra a su **ra√≠z o ra√≠z ling√º√≠stica**, que **no siempre es una palabra v√°lida en el idioma**. Por ejemplo, "corriendo", "corri√≥" y "corro" podr√≠an ser reducidas a una ra√≠z como "corr". El stemming no considera el contexto gramatical ni respeta las reglas ling√º√≠sticas, ya que est√° basado en reglas heur√≠sticas predefinidas. Esto lo hace m√°s r√°pido y computacionalmente m√°s eficiente que la lematizaci√≥n, pero tambi√©n m√°s propenso a errores.

En t√©rminos generales, el stemming es una t√©cnica m√°s agresiva y simplificada, mientras que la lematizaci√≥n es m√°s precisa y basada en el conocimiento ling√º√≠stico.

> [!note]
>
> ###### Part-of-Speech (POS) en procesamiento de lenguaje natural
>
> El etiquetado de **Part-of-Speech (POS)**, o etiquetado de categor√≠as gramaticales, es una t√©cnica fundamental en procesamiento de lenguaje natural (NLP). Consiste en asignar una categor√≠a gramatical a cada palabra de un texto, como sustantivo, verbo, adjetivo o adverbio, dependiendo de su contexto y funci√≥n dentro de la oraci√≥n.
>
> Esta tarea es esencial porque las palabras pueden tener m√∫ltiples significados o desempe√±ar diferentes roles seg√∫n su uso. Por ejemplo, en la frase "Me gusta correr", "correr" se etiqueta como un verbo, pero en "El correr de los a√±os", se clasifica como un sustantivo. El etiquetado POS ayuda a los modelos de NLP a desambiguar estas interpretaciones y comprender mejor el texto.
>
> El POS tagging suele basarse en algoritmos que combinan reglas ling√º√≠sticas con modelos estad√≠sticos o de aprendizaje profundo. Estos modelos utilizan caracter√≠sticas como el contexto de las palabras vecinas, los sufijos, los prefijos y datos previamente etiquetados para predecir la categor√≠a gramatical correcta.
>
> En la pr√°ctica, bibliotecas como **spaCy** y **NLTK** facilitan el etiquetado POS. Por ejemplo, con spaCy, al procesar la frase *"Estoy aprendiendo NLP"*, se asignar√≠an etiquetas como PRON para "Estoy", VERB para "aprendiendo" y PROPN para "NLP".
>
> El etiquetado POS tiene m√∫ltiples aplicaciones, como an√°lisis sint√°ctico, extracci√≥n de informaci√≥n y construcci√≥n de modelos m√°s avanzados que dependen de la estructura gramatical del texto. Es una herramienta clave para lograr una mejor comprensi√≥n sem√°ntica en NLP.

#### Diferencias clave entre lematizaci√≥n y stemming

Aunque ambas t√©cnicas persiguen el objetivo de simplificar las palabras, las diferencias entre ellas son claras. El stemming se basa en cortar prefijos y sufijos siguiendo reglas b√°sicas, sin importar si la ra√≠z resultante es una palabra v√°lida. Esto puede producir resultados incorrectos, como convertir "correremos" en "correrem", lo cual no tiene sentido en espa√±ol.

La lematizaci√≥n, en cambio, se basa en analizar el contexto ling√º√≠stico y aplicar un modelo de lenguaje para obtener un resultado m√°s exacto. Por ejemplo, "corriendo" y "corri√≥" ser√≠an correctamente reducidas a "correr", respetando el idioma y el significado de las palabras.

La decisi√≥n de usar lematizaci√≥n o stemming depender√° de los requerimientos del proyecto. Si se necesita rapidez y los resultados no tienen que ser perfectos, el stemming puede ser suficiente. En cambio, si la precisi√≥n y el significado son cruciales, la lematizaci√≥n ser√° la mejor opci√≥n.

#### Ejemplos de stemming y lematizaci√≥n con bibliotecas NLP en Python

##### NLTK

En NLTK, se dispone tanto de herramientas para realizar stemming como lematizaci√≥n. Para el stemming, NLTK incluye algoritmos populares como **PorterStemmer** y **SnowballStemmer**. Para la lematizaci√≥n, se puede utilizar el **WordNetLemmatizer**, que se basa en la base de datos l√©xica de WordNet.

En el siguiente ejemplo, aplicamos ambas t√©cnicas a un texto en ingl√©s:

```python
import nltk
from nltk.stem import PorterStemmer, WordNetLemmatizer
from nltk.corpus import wordnet

# Descargar los recursos necesarios
nltk.download('wordnet')
nltk.download('omw-1.4')

# Crear instancias de stemming y lematizaci√≥n
stemmer = PorterStemmer()
lemmatizer = WordNetLemmatizer()

# Texto de ejemplo
palabras = ["running", "ran", "runs", "easily", "fairness"]

# Aplicar stemming
stems = [stemmer.stem(word) for word in palabras]
print("Stemming:", stems)

# Aplicar lematizaci√≥n
lemmas = [lemmatizer.lemmatize(word, pos='v') for word in palabras]
print("Lematizaci√≥n:", lemmas)
```

**Salida esperada:**

```
Stemming: ['run', 'ran', 'run', 'easili', 'fair']
Lematizaci√≥n: ['run', 'run', 'run', 'run', 'fairness']
```

En este ejemplo, se observa que el stemming produce resultados como "easili", que no tienen sentido como palabras v√°lidas. Por otro lado, la lematizaci√≥n identifica correctamente el verbo base "run", pero respeta la forma original de "fairness", ya que es un sustantivo.

##### spaCy

SpaCy incluye la lematizaci√≥n como una caracter√≠stica integrada en sus modelos de procesamiento. A diferencia de NLTK, no incluye stemming, ya que prioriza enfoques ling√º√≠sticamente precisos.

El siguiente ejemplo muestra c√≥mo lematizar un texto en espa√±ol utilizando spaCy:

```python
import spacy

# Cargar el modelo de espa√±ol
# Nota: Si no tienes el modelo, inst√°lalo con: python -m spacy download es_core_news_sm
nlp = spacy.load("es_core_news_sm")

# Texto de ejemplo
texto = "Estoy corriendo y ellos corrieron ayer."

# Procesar el texto con spaCy
doc = nlp(texto)

# Extraer los lemas
lemmas = [token.lemma_ for token in doc]
print("Lemas:", lemmas)
```

**Salida esperada:**

```
Lemas: ['estar', 'correr', 'y', 'ello', 'correr', 'ayer', '.']
```

En este caso, spaCy identifica correctamente los lemas de las palabras en espa√±ol, como "correr" para "corriendo" y "corrieron". Adem√°s, es capaz de reconocer conectores como "y" y transformarlos en su lema base, si corresponde.

> [!warning]
>
> La elecci√≥n entre lematizaci√≥n y stemming depende en gran medida del equilibrio entre precisi√≥n y eficiencia que se requiera en un proyecto. Mientras que el stemming puede ser adecuado en tareas preliminares o con limitaciones computacionales, la lematizaci√≥n es la opci√≥n preferida para aplicaciones que demandan un an√°lisis profundo del lenguaje.



##### Para reflexionar...

> **¬øPor qu√© el stemming podr√≠a ser insuficiente para proyectos de NLP en idiomas como el espa√±ol?**
>  **Clave**: Considera c√≥mo la complejidad morfol√≥gica de un idioma afecta la capacidad del stemming para reducir palabras a una forma base.



### Normalizaci√≥n avanzada en NLP: Manejo de sin√≥nimos y estandarizaci√≥n sem√°ntica

Cuando hablamos de normalizaci√≥n avanzada en NLP, adem√°s de las fases b√°sicas como la lematizaci√≥n, eliminaci√≥n de stop words o conversi√≥n a min√∫sculas, es importante considerar c√≥mo abordar variaciones m√°s complejas en el significado del texto. Estas variaciones pueden incluir la presencia de sin√≥nimos, redundancias sem√°nticas o inconsistencias en la forma en la que se expresan ciertos t√©rminos o conceptos. Abordar estas cuestiones es clave para proyectos en los que el an√°lisis del contenido necesita precisi√≥n sem√°ntica o uniformidad conceptual.

Un aspecto fundamental de esta normalizaci√≥n es el manejo de sin√≥nimos. En el lenguaje natural, es com√∫n que un mismo concepto pueda ser expresado de m√∫ltiples maneras. Por ejemplo, t√©rminos como "autom√≥vil", "coche" y "veh√≠culo" son sin√≥nimos que representan la misma idea en la mayor√≠a de los contextos. Si no se manejan adecuadamente, estos t√©rminos pueden ser tratados como entidades distintas, lo que puede llevar a modelos ineficientes o interpretaciones incorrectas. Para resolver este problema, se pueden utilizar diccionarios o bases de datos sem√°nticas como **WordNet** en ingl√©s o recursos similares para otros idiomas. Estas herramientas permiten mapear palabras diferentes a un mismo concepto base.

Otra t√©cnica avanzada es la detecci√≥n y correcci√≥n de redundancias sem√°nticas en el texto. Frases como "subir arriba" o "bajar hacia abajo" no aportan informaci√≥n adicional y generan ruido innecesario. Detectar estos patrones es esencial para mejorar la calidad del texto y evitar problemas en tareas como la extracci√≥n de informaci√≥n o la generaci√≥n de res√∫menes. Las t√©cnicas basadas en an√°lisis de dependencias sint√°cticas, disponibles en bibliotecas como **spaCy**, pueden ser √∫tiles para identificar este tipo de redundancias.

Adem√°s, la estandarizaci√≥n de t√©rminos y formatos es otro aspecto clave de la normalizaci√≥n avanzada. En ciertas aplicaciones, es importante que los t√©rminos sean consistentes para facilitar la interpretaci√≥n. Por ejemplo, en un sistema de procesamiento de datos financieros, "EUR", "‚Ç¨", o "euro" deben ser normalizados a una misma representaci√≥n, como "EUR". De manera similar, las fechas o unidades de medida tambi√©n necesitan ser estandarizadas para evitar inconsistencias.

#### Manejo de sin√≥nimos con NLTK y WordNet

La biblioteca **NLTK** incluye la base de datos l√©xica **WordNet**, que permite identificar sin√≥nimos y palabras relacionadas en ingl√©s. Veamos c√≥mo mapear sin√≥nimos a una representaci√≥n uniforme:

```python
from nltk.corpus import wordnet

# Descargar WordNet
import nltk
nltk.download('wordnet')

# Texto de ejemplo
texto = "The car is parked near the automobile showroom."

# Normalizaci√≥n de sin√≥nimos
def normalizar_sinonimos(palabra):
    sinonimos = wordnet.synsets(palabra)
    if sinonimos:
        # Retorna el lema del primer sin√≥nimo encontrado
        return sinonimos[0].lemmas()[0].name()
    return palabra

# Normalizaci√≥n del texto
palabras = texto.split()
texto_normalizado = " ".join([normalizar_sinonimos(palabra.lower()) for palabra in palabras])
print("Texto normalizado:", texto_normalizado)
```

**Salida esperada:**

```
Texto normalizado: the car is parked near the car showroom
```

En este ejemplo, el t√©rmino "automobile" fue reconocido como sin√≥nimo de "car" y normalizado a esta forma base. Este enfoque puede extenderse a otras palabras relacionadas para mejorar la uniformidad sem√°ntica del texto.

#### Estandarizaci√≥n de t√©rminos con expresiones regulares

En ciertas aplicaciones, es com√∫n trabajar con textos que contienen formatos variables, como fechas, monedas o unidades de medida. Las expresiones regulares pueden ser una herramienta eficaz para detectar y unificar estos elementos.

```python
import re

# Texto de ejemplo
texto = "El precio del producto es $500. Tambi√©n se acepta US$ 500 o 500 d√≥lares."

# Normalizaci√≥n de formatos monetarios a "USD 500"
texto_normalizado = re.sub(r'\$|US\$|d√≥lares', 'USD', texto)
print("Texto normalizado:", texto_normalizado)
```

**Salida esperada:**

```
Texto normalizado: El precio del producto es USD 500. Tambi√©n se acepta USD 500 o 500 USD.
```

En este caso, todas las variaciones de representaci√≥n monetaria se normalizaron a un formato est√°ndar, lo que facilita el an√°lisis automatizado.

#### Uso de t√©cnicas avanzadas de normalizaci√≥n con spaCy

##### Manejo de sin√≥nimos con spaCy y bases de datos personalizadas

Aunque spaCy no incluye un recurso de sin√≥nimos como **WordNet**, podemos integrarlo f√°cilmente con bases de datos externas o crear un diccionario personalizado de sin√≥nimos. Esto permite mapear t√©rminos equivalentes a una representaci√≥n uniforme.

Por ejemplo, supongamos que tienes un diccionario de sin√≥nimos en espa√±ol. Este puede integrarse al procesamiento del texto con spaCy:

```python
import spacy

# Cargar el modelo de spaCy
nlp = spacy.load("es_core_news_sm")

# Diccionario de sin√≥nimos
diccionario_sinonimos = {
    "autom√≥vil": "coche",
    "veh√≠culo": "coche",
    "carro": "coche",
    "computadora": "ordenador",
    "port√°til": "laptop",
}

# Funci√≥n para normalizar sin√≥nimos
def normalizar_sinonimos(token):
    if token.text.lower() in diccionario_sinonimos:
        return diccionario_sinonimos[token.text.lower()]
    return token.text

# Texto de ejemplo
texto = "Compr√© un autom√≥vil nuevo y ahora estoy buscando una computadora port√°til."

# Procesar el texto
doc = nlp(texto)

# Normalizar los sin√≥nimos
texto_normalizado = " ".join([normalizar_sinonimos(token) for token in doc])
print("Texto normalizado:", texto_normalizado)
```

**Salida esperada:**

```
Texto normalizado: Compr√© un coche nuevo y ahora estoy buscando una ordenador laptop.
```

En este ejemplo, las palabras "autom√≥vil" y "computadora port√°til" fueron mapeadas a sus equivalentes en el diccionario. Esta aproximaci√≥n puede extenderse f√°cilmente seg√∫n las necesidades del dominio del proyecto.

##### Detecci√≥n de redundancias sem√°nticas con spaCy

SpaCy es ideal para trabajar con an√°lisis de dependencias sint√°cticas, lo que permite detectar redundancias sem√°nticas en las oraciones. Por ejemplo, frases como "subir arriba" o "bajar hacia abajo" pueden ser identificadas al analizar las relaciones entre palabras.

Veamos un ejemplo pr√°ctico:

```python
import spacy

# Cargar el modelo en espa√±ol
nlp = spacy.load("es_core_news_sm")

# Texto de ejemplo con redundancias
texto = "Voy a subir arriba y luego bajar hacia abajo."

# Procesar el texto
doc = nlp(texto)

# Detectar redundancias sem√°nticas
redundancias = []
for token in doc:
    if token.dep_ == "obl" and token.head.lemma_ in ["subir", "bajar"]:
        redundancias.append(f"{token.head.text} {token.text}")

print("Redundancias detectadas:", redundancias)
```

**Salida esperada:**

```
Redundancias detectadas: ['subir arriba', 'bajar abajo']
```

En este ejemplo, spaCy identifica las redundancias sem√°nticas analizando las dependencias entre las palabras. Esto permite eliminarlas o corregirlas de manera program√°tica en un paso posterior.

##### Estandarizaci√≥n de t√©rminos y formatos con spaCy

La estandarizaci√≥n de t√©rminos, como unidades, monedas o formatos de fechas, puede realizarse utilizando las capacidades de spaCy para el reconocimiento de entidades nombradas (NER, por sus siglas en ingl√©s). Con esta t√©cnica, podemos identificar conceptos clave y normalizarlos seg√∫n un formato predefinido.

Por ejemplo, para estandarizar monedas y convertirlas a un formato com√∫n como "USD":

```python
import spacy

# Cargar el modelo en ingl√©s (spaCy es especialmente bueno con NER en ingl√©s)
nlp = spacy.load("en_core_web_sm")

# Texto de ejemplo con diferentes formatos de moneda
texto = "The product costs $500. Alternatively, you can pay 500 dollars or USD 500."

# Procesar el texto
doc = nlp(texto)

# Estandarizar entidades de moneda
texto_normalizado = []
for token in doc:
    if token.ent_type_ == "MONEY":
        texto_normalizado.append("USD")
    else:
        texto_normalizado.append(token.text)

# Reconstruir el texto normalizado
texto_normalizado = " ".join(texto_normalizado)
print("Texto normalizado:", texto_normalizado)
```

**Salida esperada:**

```
Texto normalizado: The product costs USD . Alternatively , you can pay USD or USD .
```

En este ejemplo, spaCy utiliza su modelo de reconocimiento de entidades nombradas para identificar expresiones relacionadas con dinero (como "$500" o "500 dollars") y las estandariza al formato "USD".

> [!warning]
>
> Adem√°s de spaCy o NLTK, existen otras bibliotecas √∫tiles para tareas espec√≠ficas de normalizaci√≥n:
>
> - **TextBlob**: Puede usarse para correcci√≥n ortogr√°fica y detecci√≥n de sin√≥nimos en textos m√°s simples.
> - **SymSpell**: Es ideal para la correcci√≥n ortogr√°fica eficiente en textos grandes.
> - **FuzzyWuzzy**: Ayuda a encontrar palabras similares o realizar emparejamiento aproximado, √∫til para identificar t√©rminos mal escritos o sin√≥nimos impl√≠citos.
>
> Por ejemplo, para usar **SymSpell** en la correcci√≥n de palabras mal escritas:
>
> ```python
> from symspellpy import SymSpell, Verbosity
> 
> # Inicializar SymSpell
> sym_spell = SymSpell(max_dictionary_edit_distance=2)
> sym_spell.create_dictionary_entry("autom√≥vil", 1)
> sym_spell.create_dictionary_entry("veh√≠culo", 1)
> 
> # Texto con errores
> texto = "Compr√© un autoovil nuevo."
> 
> # Correcci√≥n ortogr√°fica
> correcciones = sym_spell.lookup("autoovil", Verbosity.CLOSEST, max_edit_distance=2)
> correccion = correcciones[0].term if correcciones else "Sin correcci√≥n"
> 
> print("Correcci√≥n:", correccion)
> ```
>
> **Salida esperada:**
>
> ```
> Correcci√≥n: autom√≥vil
> ```

---

El uso de t√©cnicas avanzadas de normalizaci√≥n con NLTK, spaCy u otras bibliotecas ampl√≠a significativamente las capacidades de preprocesamiento de texto, permitiendo abordar problemas m√°s complejos como la variabilidad sem√°ntica, las redundancias y las inconsistencias en formatos. Estas herramientas son esenciales para mejorar la calidad de los datos y garantizar que los modelos de NLP trabajen con representaciones m√°s uniformes y significativas.

Las t√©cnicas avanzadas de normalizaci√≥n son especialmente √∫tiles en aplicaciones donde la uniformidad del texto es cr√≠tica. En sistemas de b√∫squeda sem√°ntica, el manejo de sin√≥nimos garantiza que los usuarios obtengan resultados relevantes sin importar las palabras que utilicen para formular sus consultas. Por otro lado, la estandarizaci√≥n de t√©rminos es esencial en dominios como finanzas o ciencia, donde las inconsistencias en unidades o formatos pueden conducir a errores significativos en los modelos.

Adem√°s, la correcci√≥n de redundancias sem√°nticas es crucial en la generaci√≥n de res√∫menes o simplificaci√≥n de textos, asegurando que el contenido final sea claro y directo. En general, estas t√©cnicas mejoran tanto la calidad de los datos como el rendimiento de los modelos en proyectos avanzados de NLP.



##### Para reflexionar...

> **¬øQu√© limitaciones podr√≠an surgir al usar diccionarios de sin√≥nimos predefinidos para normalizar textos en idiomas con una alta variabilidad regional, como el espa√±ol?**
>  **Clave**: Piensa en c√≥mo las diferencias culturales, los dialectos y el contexto espec√≠fico de cada dominio pueden influir en la selecci√≥n de sin√≥nimos.
