# Tema 4. Procesamiento de lenguaje natural y LLM (Large Language Models)

## Limpieza y preprocesamiento de textos

1. Introducci√≥n
1. Limpieza de textos

---

### Introducci√≥n

En cualquier proyecto de procesamiento del lenguaje natural (PLN), antes de llegar a la codificaci√≥n del texto, es fundamental pasar por una serie de fases de **limpieza** y **preprocesamiento**. Estas etapas aseguran que los datos est√©n en un formato adecuado para el an√°lisis y que se reduzca el ruido, maximizando la utilidad del texto para los modelos de aprendizaje autom√°tico. El resto de este cap√≠tulo se dedicar√° a explicar cu√°les son cada una de las mencionadas fases y de qu√© manera podemos implementarlas desde un punto de vista pr√°ctico. Pero primeramente enumeremos las fases en las que podemos dividir un proceso de limpieza y preprocesamiento de textos:

En lo que respecta a la **limpieza del texto** podemos distinguir entre:

1. **Eliminaci√≥n de caracteres no deseados**: Eliminaci√≥n de signos de puntuaci√≥n, caracteres especiales, n√∫meros u otros elementos irrelevantes para la tarea.
2. **Conversi√≥n a min√∫sculas**: Normalizaci√≥n del texto para evitar que palabras en distinto caso sean tratadas como diferentes.
3. **Eliminaci√≥n de espacios innecesarios**: Eliminaci√≥n de espacios adicionales que puedan generar inconsistencias en los datos.
4. **Correcci√≥n ortogr√°fica (opcional)**: En tareas espec√≠ficas, se pueden corregir errores ortogr√°ficos para mejorar la calidad del texto.

En lo que respecta al **preprocesamiento del texto** podemos distinguir entre:

1. **Eliminaci√≥n de stop words**: eliminaci√≥n de palabras muy frecuentes y con poca carga sem√°ntica.
2. **Tokenizaci√≥n**: Divisi√≥n del texto en unidades b√°sicas, ya sean palabras, frases o caracteres. Ello depender√° de la tarea concreta a abordar.
3. **Lematizaci√≥n o stemming**: Reducci√≥n de las palabras a su forma base o ra√≠z para normalizar el vocabulario.
4. **Normalizaci√≥n adicional (opcional)**: Sustituci√≥n de palabras sin√≥nimas o aplicaci√≥n de t√©cnicas espec√≠ficas que reduzcan la complejidad del texto.
5. **Filtrado de t√©rminos irrelevantes**: En algunos casos, es √∫til eliminar palabras espec√≠ficas que no sean √∫tiles para la tarea en cuesti√≥n (como nombres propios o t√©rminos t√©cnicos irrelevantes).

Todas las fases enumeradas anteriormente deben ser realizadas en el orden adecuado y adaptadas a los requisitos espec√≠ficos del problema que se est√° resolviendo. Una vez completadas, el texto estar√° listo para ser codificado en un formato num√©rico que los modelos puedan interpretar.

### Eliminaci√≥n de caracteres no deseados

La **eliminaci√≥n de caracteres no deseados** es una de las primeras fases en el proceso de limpieza de texto en proyectos de procesamiento del lenguaje natural (PLN). Este paso tiene como objetivo eliminar caracteres y elementos concretos que no aportan informaci√≥n relevante para la tarea espec√≠fica, reduciendo el ruido y asegurando que el texto est√© en un formato adecuado para su posterior procesamiento. Estos caracteres incluyen, entre otros, signos de puntuaci√≥n, caracteres especiales, n√∫meros (en algunos casos), y secuencias de espacios en blanco. De todos modos, es la naturaleza de la tarea la que marcar√° las necesidades en cuanto a la eliminaci√≥n o no de caracteres concretos y s√≠mbolos.

En efecto, aunque a primera vista puede parecer un paso sencillo, la eliminaci√≥n de caracteres no deseados debe realizarse con cuidado, ya que puede haber informaci√≥n valiosa escondida en algunos de ellos seg√∫n el contexto y la tarea. Por ejemplo, mientras los signos de puntuaci√≥n pueden ser irrelevantes en un an√°lisis de sentimientos, podr√≠an ser importantes en la identificaci√≥n de entidades nombradas o en la generaci√≥n de texto.

> **Ejemplo 1:** An√°lisis de sentimientos en redes sociales
>
> El objetivo en este caso es determinar el sentimiento expresado en publicaciones de redes sociales. Las publicaciones pueden incluir emojis, signos de puntuaci√≥n y hashtags, que en este contexto **aportan informaci√≥n valiosa sobre el sentimiento**. Supongamos el siguiente texto a procesar
>
> ```
> "¬°Me encanta este producto! üòä‚ù§Ô∏è #felicidad #compras"
> ```
>
> En este caso se podr√≠a proceder de la siguiente manera:
>
> 1. Conservar emojis y hashtags porque aportan informaci√≥n emocional.
> 2. Eliminar signos de puntuaci√≥n innecesarios para simplificar el texto.
>
> De este modo, tras la limpieza, el texto  quedar√≠a como:
>
> ```
> "Me encanta este producto üòä‚ù§Ô∏è felicidad compras"
> ```
>
> Vemos como en este caso, los emojis (üòä, ‚ù§Ô∏è) y los hashtags (#felicidad) son importantes porque a√±aden se√±ales emocionales y tem√°ticas. Eliminarlos podr√≠a llevar a perder informaci√≥n relevante para el an√°lisis de sentimientos.

> **Ejemplo2:** Procesamiento de textos financieros
>
> En este caso, el objetivo es extraer informaci√≥n clave de documentos financieros, como contratos o informes. Aqu√≠, elementos como n√∫meros, s√≠mbolos de moneda ($, ‚Ç¨, ¬•) y porcentajes (%) **son esenciales para la interpretaci√≥n correcta** del texto.
>
> Supongamos el siguiente texto:
>
> ```
> "El valor total del contrato es de $1,200,000 con una tasa de inter√©s del 5% anual."
> ```
>
> Los criterios a seguir podr√≠an ser:
>
> 1. Conservar n√∫meros y s√≠mbolos de moneda porque son cr√≠ticos para la tarea.
> 2. Eliminar signos de puntuaci√≥n innecesarios, como comas separadoras, para estandarizar el formato num√©rico.
>
> De este modo el texto quedar√≠a finalmente:
>
> ```
> "El valor total del contrato es de 1200000 con una tasa de inter√©s del 5% anual"
> ```
>
> En este caso, eliminar n√∫meros o s√≠mbolos de moneda como "$" habr√≠a afectado la interpretaci√≥n financiera del texto. Por el contrario, quitar las comas de los n√∫meros ayuda a mantener consistencia en el formato.

#### T√©cnicas para eliminar caracteres no deseados

##### Uso de expresiones regulares

Las **expresiones regulares** son herramientas extremadamente poderosas para buscar y manipular patrones en el texto. Permiten identificar y eliminar caracteres espec√≠ficos o grupos de caracteres seg√∫n reglas definidas.

Por ejemplo:

- Para eliminar todos los signos de puntuaci√≥n: `r'[^\w\s]'` (este patr√≥n elimina todo excepto palabras y espacios).
- Para eliminar n√∫meros: `r'\d+'`.
- Para eliminar caracteres no alfab√©ticos: `r'[^a-zA-Z\s]'`.

El uso de expresiones regulares es altamente configurable, lo que las hace ideales para tareas avanzadas.

##### Sustituci√≥n por listas predefinidas

Otra t√©cnica consiste en definir expl√≠citamente los caracteres a eliminar o conservar. Esto puede ser √∫til si se tiene un control m√°s estricto sobre el tipo de texto procesado. Por ejemplo, en un corpus t√©cnico, se podr√≠a decidir conservar ciertos caracteres especiales como "#" o "$" porque tienen un significado relevante.

##### Uso de bibliotecas especializadas en python

Python ofrece herramientas poderosas para la limpieza de texto, como las expresiones regulares a trav√©s del m√≥dulo `re`, y bibliotecas espec√≠ficas como `nltk` , `spacy` o incluso `pandas`. 

La eliminaci√≥n de caracteres no deseados es un paso crucial en la limpieza de texto y cada una de estas bibliotecas ofrece funcionalidades espec√≠ficas que se pueden adaptar seg√∫n los requisitos de la tarea y el contexto del an√°lisis.

La biblioteca **re** de Python, basada en expresiones regulares, es una de las opciones m√°s flexibles y poderosas para identificar y eliminar patrones espec√≠ficos en un texto. Con re, se pueden eliminar signos de puntuaci√≥n, n√∫meros, espacios en blanco redundantes o incluso caracteres especiales en una sola l√≠nea de c√≥digo. Por ejemplo, `re.sub(r'[^\w\s]', '', texto)` elimina todos los caracteres que no sean palabras o espacios, dejando un texto limpio y estructurado.

Por otro lado, **nltk** ofrece herramientas para una limpieza m√°s sem√°ntica. Aunque su enfoque principal est√° en el preprocesamiento, como la tokenizaci√≥n y la eliminaci√≥n de stop words, tambi√©n puede usarse para filtrar texto mediante listas predefinidas de caracteres no deseados o stop words espec√≠ficas. Esto resulta √∫til cuando se necesita un control m√°s granular en textos en m√∫ltiples idiomas o dominios.

Finalmente, **spaCy** combina eficiencia y simplicidad al proporcionar m√©todos preintegrados para manipular texto. Por ejemplo, al procesar un documento con spaCy, se pueden eliminar caracteres no deseados aprovechando sus etiquetas morfol√≥gicas, como distinguir entre puntuaci√≥n, n√∫meros y palabras relevantes. Adem√°s, spaCy permite un an√°lisis m√°s profundo del texto, como conservar palabras clave o s√≠mbolos importantes seg√∫n su uso en el corpus.

En conjunto, estas bibliotecas ofrecen enfoques complementarios, adaptables a las necesidades de cualquier proyecto de PLN. Su correcta integraci√≥n garantiza una limpieza eficiente y personalizada del texto, sentando las bases para an√°lisis precisos y efectivos.

> [!note]
>
> La eliminaci√≥n de caracteres no deseados es un paso cr√≠tico en la limpieza de texto. Su correcta implementaci√≥n asegura que el texto est√© libre de ruido y listo para las etapas posteriores de preprocesamiento y an√°lisis. Dependiendo del contexto y la tarea, las t√©cnicas deben ajustarse cuidadosamente para no perder informaci√≥n relevante. Herramientas como expresiones regulares y bibliotecas de Python como `re`, `nltk` o `spacy` permiten realizar esta tarea de manera eficiente, incluso en corpus de gran tama√±o. Este paso, aunque inicial, sienta las bases para un procesamiento del lenguaje m√°s preciso y efectivo.

### Unificaci√≥n del caso (conversi√≥n a min√∫sculas o may√∫sculas)

La **conversi√≥n o unificaci√≥n del caso**, es decir, transformar todo el texto a min√∫sculas o may√∫sculas, es una de las operaciones m√°s b√°sicas pero esenciales en el proceso de limpieza de texto en proyectos de procesamiento del lenguaje natural (PLN). Este paso tiene como objetivo normalizar el texto, garantizando, por ejemplo, que palabras como "Casa" y "casa" no sean tratadas como diferentes por los algoritmos. Aunque las letras may√∫sculas pueden aportar significado en ciertos contextos, como nombres propios o siglas, para muchas tareas de PLN esta distinci√≥n no es relevante, y la conversi√≥n a min√∫sculas simplifica el an√°lisis y mejora la consistencia de los datos.

El valor de esta etapa depende de la tarea espec√≠fica. En proyectos como an√°lisis de sentimientos, clasificaci√≥n de texto o generaci√≥n de lenguaje, la conversi√≥n a min√∫sculas es casi siempre √∫til, ya que evita que el modelo tenga que aprender m√∫ltiples representaciones para palabras id√©nticas, como "Apple" y "apple". Sin embargo, en tareas como la extracci√≥n de entidades nombradas, puede ser preferible conservar las may√∫sculas, ya que aportan informaci√≥n sobre nombres propios, t√≠tulos o siglas.

> **Ejemplo 1**: An√°lisis de sentimientos en rese√±as de productos
>
> En un an√°lisis de sentimientos, el objetivo es determinar si el texto refleja una opini√≥n positiva, negativa o neutral. En este caso, las may√∫sculas no aportan informaci√≥n adicional sobre el sentimiento.
>
> Supongamos el siguiente texto
>
> ```
> "¬°Este Producto es Fant√°stico! Muy recomendable."
> ```
>
> Se podr√≠a convertir todo el texto a min√∫sculas para evitar redundancias en las representaciones de palabras. De este modo el texto final quedar√≠a como:
>
> ```
> "¬°este producto es fant√°stico! muy recomendable."
> ```

> **Ejemplo 2:** Extracci√≥n de entidades nombradas (NER)
>
> En tareas de extracci√≥n de entidades nombradas, como identificar nombres de personas, empresas o ubicaciones, conservar las may√∫sculas puede ser crucial, ya que estas suelen ser un indicador de nombres propios. Por ejemplo, supongamos el siguiente texto:
>
> ```
> "Microsoft anunci√≥ una colaboraci√≥n con Apple Inc."
> ```
>
> En este caso, se podr√≠a optar por no convertir el texto a min√∫sculas para preservar informaci√≥n sobre entidades nombradas. De este modo el texto quedar√≠a sin alteraci√≥n:
>
> ```
> "Microsoft anunci√≥ una colaboraci√≥n con Apple Inc."
> ```

#### T√©cnicas para la conversi√≥n del caso

##### Conversi√≥n a min√∫sculas o may√∫sculas con python "puro"

Python proporciona m√©todos sencillos para convertir texto a min√∫sculas. La funci√≥n `lower()` se utiliza para transformar cada car√°cter alfab√©tico en su equivalente en min√∫sculas, mientras que otros caracteres permanecen inalterados. Por ejemplo:

```python
texto = "¬°Este Producto es Fant√°stico!"
texto_min = texto.lower()
print(texto_min)
# Salida: "¬°este producto es fant√°stico!"
```

Para tareas espec√≠ficas, como generaci√≥n de encabezados o formateo de texto en documentos oficiales, se puede utilizar la funci√≥n `upper()` para transformar todo el texto a may√∫sculas. Por ejemplo:

```python
texto = "¬°Este Producto es Fant√°stico!"
texto_may = texto.upper()
print(texto_may)
# Salida: "¬°ESTE PRODUCTO ES FANT√ÅSTICO!"
```

Adem√°s de convertir todo el texto a min√∫sculas o may√∫sculas, es posible utilizar m√©todos como `capitalize()` para convertir solo la primera letra de una oraci√≥n en may√∫scula, o `title()` para capitalizar cada palabra. Esto es √∫til en tareas de formateo. Por ejemplo:

```python
texto = "¬°este producto es fant√°stico!"
texto_capitalizado = texto.capitalize()
print(texto_capitalizado)
# Salida: "¬°este producto es fant√°stico!"

texto_titulo = texto.title()
print(texto_titulo)
# Salida: "¬°Este Producto Es Fant√°stico!"
```

##### Uso de bibliotecas especializadas en Python

Aunque las funciones integradas de Python son √∫tiles, bibliotecas como **nltk** y **spaCy** ofrecen herramientas adicionales para manejar la conversi√≥n del caso de manera m√°s robusta en corpus grandes o multiling√ºes.

Por su lado, **NLTK** aunque no incluye funciones espec√≠ficas para cambiar el caso, su compatibilidad con procesamiento de texto tokenizado permite integrar f√°cilmente la conversi√≥n del caso con otras etapas de limpieza.

Podemos verlo en el siguiente ejemplo:

```python
from nltk.tokenize import word_tokenize
import nltk
nltk.download('punkt')

texto = "¬°Este Producto es Fant√°stico!"
tokens = word_tokenize(texto.lower())
print(tokens)
# Salida: ['¬°', 'este', 'producto', 'es', 'fant√°stico', '!']
```

Por otro lado, **spaCy** ofrece una forma eficiente de procesar texto mientras mantiene el control sobre el caso. Permite aplicar conversiones en palabras individuales mientras se conserva informaci√≥n estructural.

Por ejemplo:

```python
import spacy

nlp = spacy.load("es_core_news_sm")
doc = nlp("¬°Este Producto es Fant√°stico!")
texto_min = " ".join([token.text.lower() for token in doc])
print(texto_min)
# Salida: "¬°este producto es fant√°stico!"
```

> [!note]
>
> La conversi√≥n del caso es un paso sencillo pero fundamental en el preprocesamiento del texto. Aunque a menudo la conversi√≥n a min√∫sculas es suficiente para garantizar la consistencia y reducir redundancias, la decisi√≥n de aplicar esta t√©cnica debe basarse en el contexto de la tarea. Por ejemplo, en an√°lisis de sentimientos, las may√∫sculas suelen ser irrelevantes, mientras que en la extracci√≥n de entidades nombradas o textos donde las siglas son importantes, conservar el caso original puede ser cr√≠tico. Herramientas como Python, nltk y spaCy permiten implementar estas conversiones de manera eficiente y adaptada a las necesidades del proyecto, garantizando resultados consistentes y precisos.

### Eliminaci√≥n de espacios innecesarios: Limpieza para la consistencia del texto

La **eliminaci√≥n de espacios innecesarios** es otro paso importante en la limpieza de texto en proyectos de procesamiento del lenguaje natural. Aunque puede parecer un detalle menor, los espacios redundantes o mal colocados pueden generar inconsistencias en los datos, lo que afecta tanto la representaci√≥n del texto como los resultados de los modelos. As√≠, esta fase tiene como objetivo normalizar la estructura del texto, asegurando que las palabras y frases est√©n correctamente delimitadas por un √∫nico espacio, sin interferencias de caracteres invisibles como tabulaciones o saltos de l√≠nea innecesarios.

En muchas tareas de NLP, los espacios adicionales pueden introducir ruido. Por ejemplo, palabras separadas por m√∫ltiples espacios podr√≠an ser interpretadas de manera err√≥nea por un modelo, afectando la tokenizaci√≥n o el an√°lisis de patrones. Adem√°s, en textos extra√≠dos de fuentes diversas como formularios, bases de datos o archivos PDF, los espacios innecesarios suelen ser un problema recurrente debido al formato del documento o a errores en la extracci√≥n del texto.

Sin embargo, la eliminaci√≥n de espacios tambi√©n debe realizarse con cuidado en contextos donde la ubicaci√≥n del espacio tiene un significado sem√°ntico o estructural, como en formatos tabulares o ciertas expresiones matem√°ticas.

#### T√©cnicas para eliminar espacios innecesarios

##### Uso de m√©todos b√°sicos de Python

Python proporciona m√©todos integrados simples para manejar espacios en el texto. La funci√≥n `strip()` elimina espacios al inicio y al final de una cadena, mientras que `split()` y `join()` pueden usarse en conjunto para eliminar m√∫ltiples espacios entre palabras.

Podemos ver un ejemplo ilustrativo

```python
texto = "   Me    encanta   este     producto.   "

# Eliminar espacios al inicio y al final
texto_limpio = texto.strip()

# Reducir m√∫ltiples espacios a uno solo
texto_limpio = " ".join(texto_limpio.split())

print(texto_limpio)
# Salida: "Me encanta este producto."
```

##### Uso de expresiones regulares

Para un control m√°s preciso, las expresiones regulares con la biblioteca `re` permiten identificar y reemplazar patrones de espacios redundantes. Por ejemplo:

```python
import re

texto = "   Me    encanta   este     producto.   "

# Eliminar espacios al inicio y al final
texto = texto.strip()

# Reemplazar m√∫ltiples espacios con un solo espacio
texto_limpio = re.sub(r'\s+', ' ', texto)

print(texto_limpio)
# Salida: "Me encanta este producto."
```

##### Uso de bibliotecas especializadas como `nltk` y `spaCy`

Bibliotecas como **nltk** y **spaCy** no ofrecen directamente funciones espec√≠ficas para la eliminaci√≥n de espacios, pero su integraci√≥n en el preprocesamiento facilita el manejo de texto limpio y tokenizado.

Veamos un par de ejemplos: 

```python
from nltk.tokenize import word_tokenize
import nltk
nltk.download('punkt')

texto = "   Me    encanta   este     producto.   "

# Tokenizaci√≥n con eliminaci√≥n de espacios adicionales
tokens = word_tokenize(texto)
texto_limpio = " ".join(tokens)

print(texto_limpio)
# Salida: "Me encanta este producto ."
```

```python
import spacy

nlp = spacy.load("es_core_news_sm")
texto = "   Me    encanta   este     producto.   "

# Procesar texto con spaCy
doc = nlp(texto.strip())
texto_limpio = " ".join([token.text for token in doc])

print(texto_limpio)
# Salida: "Me encanta este producto ."
```

> [!note]
>
> La eliminaci√≥n de espacios innecesarios es un paso sencillo pero crucial en la limpieza del texto. Este proceso asegura que los datos sean consistentes y libres de ruido, lo que facilita la tokenizaci√≥n y el an√°lisis posterior. Sin embargo, debe adaptarse al contexto de la tarea. En casos generales, eliminar espacios adicionales mejora la calidad del texto; sin embargo, en situaciones como la manipulaci√≥n de datos tabulares, se debe proceder con cuidado para no alterar estructuras importantes. Python, con sus herramientas integradas y bibliotecas especializadas como `nltk` y `spaCy`, ofrece m√∫ltiples enfoques para implementar esta t√©cnica de manera eficiente y precisa.

